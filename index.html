<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
  <title>Evolving Curricula</title>
  <script>
    console.log = function() {};/
    console.groupCollapsed = function() {};
    console.debug = function() {};
  </script>
  <script src="dist/template.v2.js"></script>

<!--   <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x"
        crossorigin="anonymous"> -->
  <link rel="stylesheet" href="dist/bootstrap.forms.css">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-gtEjrD/SeCtmISkJkNUaaKMoLD0//ElJ19smozuHV6z3Iehds+3Ulb9Bn9Plx0x4"
          crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.6/d3.min.js" charset="utf-8"></script>

  <script defer src="dist/demo/js/multirange.js"></script>
  <link rel="stylesheet" href="dist/multirange.css">

  <script defer src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/p5@1.2.0/lib/p5.js"></script>
  <script src=https://cdnjs.cloudflare.com/ajax/libs/seedrandom/2.3.10/seedrandom.min.js></script>

  <script src="https://cdn.jsdelivr.net/npm/intro.js@4.1.0/intro.min.js"></script>

  <!--SCRIPTS DEPENDENCIES-->
  <script src="dist/demo/js/box2d.js"></script>

  <script src="dist/demo/js/Box2D_dynamics/water_dynamics.js"></script>
  <script defer src="dist/demo/js/Box2D_dynamics/climbing_dynamics.js"></script>
  <script defer src="dist/demo/js/Box2D_dynamics/contact_detector.js"></script>
  <script defer src="dist/demo/js/utils/custom_user_data.js"></script>

  <!-- Morphologies -->
  <script defer src="dist/demo/js/bodies/bodies_enum.js"></script>
  <script defer src="dist/demo/js/bodies/abstract_body.js"></script>
  <script defer src="dist/demo/js/bodies/walkers/walker_abstract_body.js"></script>
  <script defer src="dist/demo/js/bodies/walkers/classic_bipedal_body.js"></script>
  <script defer src="dist/demo/js/bodies/walkers/spider_body.js"></script>
  <script defer src="dist/demo/js/bodies/climbers/climber_abstract_body.js"></script>
  <script defer src="dist/demo/js/bodies/climbers/climbing_profile_chimpanzee.js"></script>
  <script defer src="dist/demo/js/bodies/swimmers/swimmer_abstract_body.js"></script>
  <script defer src="dist/demo/js/bodies/swimmers/fish_body.js"></script>

  <script defer src="dist/demo/js/CPPN/cppn.js"></script>
  <script defer src="dist/demo/js/envs/multi_agents_continuous_parkour.js"></script>
  <script defer src="dist/demo/js/game.js"></script>
  <script defer src="dist/demo/js/draw_p5js.js"></script>
  <script defer src="dist/demo/js/i18n.js"></script>

  <script defer src="dist/demo/js/index.js"></script>
  <script type="module" src="dist/demo/js/ui.js"></script>

  <link rel="stylesheet" href="dist/demo.css">

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-WNMXL1RDKH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-WNMXL1RDKH');
  </script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">

</head>

<body>
  <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "Evolving Curricula with Regret-Based Environment Design",
    "description": "Combining evolution with regret-based curricula to produce highly robust agents.",
    "published": "March 3, 2022",
    "authors": [
      {
        "author":"Jack Parker-Holder<sup>*</sup>",
        "authorURL":"https://twitter.com/jparkerholder",
        "affiliations": [
          {"name": "Oxford University", "url":"https://www.robots.ox.ac.uk/~parg/"}]
      },
      {
        "author":"Minqi Jiang<sup>*</sup>",
        "authorURL":"https://twitter.com/minqijiang",
        "affiliations": [
          {"name": "UCL", "url": "https://dark.cs.ucl.ac.uk/"},
          {"name": "Meta AI", "url":"https://ai.facebook.com/"}
        ]
      },
      {
        "author":"Michael Dennis",
        "authorURL":"https://twitter.com/MichaelD1729",
        "affiliations": [
          {"name": "UC Berkeley", "url": "https://humancompatible.ai/people#people"}
        ]
      },
      {
        "author":"Mikayel Samvelyan",
        "authorURL":"https://twitter.com/samveIyan",
        "affiliations": [
          {"name": "UCL", "url": "https://dark.cs.ucl.ac.uk/"},
          {"name": "Meta AI", "url":"https://ai.facebook.com/"}
        ]
      },
      {
        "author":"Jakob Foerster",
        "authorURL":"https://twitter.com/j_foerst",
        "affiliations": [
          {"name": "Oxford University", "url": "https://www.st-annes.ox.ac.uk/cpt_people/foerster-dr-jakob/"}
        ]
      },
      {
        "author":"Edward Grefenstette",
        "authorURL":"https://twitter.com/egrefen",
        "affiliations": [
          {"name": "UCL", "url": "https://dark.cs.ucl.ac.uk/"},
          {"name": "Meta AI", "url":"https://ai.facebook.com/"}
        ]
      },
      {
        "author":"Tim Rocktäschel",
        "authorURL":"https://twitter.com/_rockt",
        "affiliations": [
          {"name": "UCL", "url": "https://dark.cs.ucl.ac.uk/"},
          {"name": "Meta AI", "url":"https://ai.facebook.com/"}
        ]
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
  <div id='cover'>
    <video id="cover-video" autoplay muted loop playsinline>
      <source src="dist/video/accel-bipedal-cover2.mp4" type="video/mp4">
    </video>
    <div id="cover-video-tint"></div>
    <d-title id="title">
  <!--     <figure style="grid-column: page; margin: 1rem 0;"><img src="momentum.png"
          style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);" /></figure> -->
      <p></p>
    </d-title>
  </div>
  <d-byline></d-byline>

  <d-article>
    <!-- <a class="marker" href="#section-1" id="section-1"><span>1</span></a> -->
    <h2>Overview</h2>
    <p>
    We evolve environments at the frontier of a reinforcement learning agent's capabilities, leading to self-supervised teacher-student processes with strong zero-shot generalization results for agents learning to walk through challenging terrain, and navigating complex human-designed mazes.
    </p>

    <!-- Canvas -->
    <div id="accel-demo">
      <div id="canvas_container"></div>

    <!-- Main buttons row -->
      <div id="mainButtons" class="row justify-content-center">
        <!-- Agents menu list -->
        <div id="env-control-panel">
          <div id="agents-menu-container">
            <ul id="agents-menu"></ul>
          </div>

          <!-- Environment controls -->
          <ul id="env-controls">
            <li>
              <div>
              <div class="slider-label">Roughness</div>
              <div class="slider-value" id="ground-roughness-value">1</div>
              </div>
              <input id="ground-roughness-slider" type="range" min="0" max="15" multiple value="1,1" />
            </li>

            <li>
              <div>
              <div class="slider-label">Stump height</div>
              <div class="slider-value" id="stump-height-value">1 - 3</div>
              </div>
              <input id="stump-height-slider" type="range" min="0" max="4" multiple value="1,3" />
            </li>

            <li>
              <div>
              <div class="slider-label">Pit gap</div>
              <div class="slider-value" id="pit-gap-value">3 - 5</div>
              </div>
              <input id="pit-gap-slider" type="range" min="0" max="5" multiple value="3,5" />
            </li>

            <li>
              <div>
              <div class="slider-label">Stair steps</div>
              <div class="slider-value" id="stair-steps-value">3 - 5</div>
              </div>
              <input id="stair-steps-slider" type="range" min="0" max="10" multiple value="3,5" />
            </li>
          </ul>

          <div id="radar-chart"></div>
        </div>

        <!-- Run/reset buttons -->
        <div id="env-buttons-container">
          <div class="col-auto py-2 px-1">
              <button id="runButton" class="btn btn-success" title="Run the simulation"><span class="fas fa-play">Run</span></button>
          </div>
          <div class="col-auto py-2 px-1">
              <button id="resetButton" class="btn btn-danger" title="Reset the simulation"><span class="fas fa-undo-alt fa-lg">Reset</span></button>
          </div>
        </div>
      </div>
      <figcaption>Interactive demo. Design your own challenging levels on which to compare an ACCEL agent to baseline methods.</figcaption>
    </div>

    <h2>Introduction</h2>

    <p>
    Deep reinforcement learning (RL) has seen tremendous success over the past decade. However, agents trained on fixed environments are brittle, often failing the moment the environment changes even slightly, thus limiting the real-world applicability of current RL methods. A common remedy is to introduce more training data diversity by randomizing the environment’s parameters in every episode—a process called domain randomization (DR). For example, these parameters might control the friction coefficient and lighting conditions in a robotic arm simulation or the position of obstacles in a maze (we call each such environment variation a level). Procedurally generating training levels to expose RL agents to more diverse experiences has quickly become a common method for improving the robustness of RL agents. However, DR is often not enough to train robust agents in domains where the agent struggles to make progress on many challenging levels. 
    </p>

    <p>
    Adaptive curriculum methods match the complexity of training levels to an agent’s current capabilities, and have been shown to produce more robust policies in fewer training steps than domain randomization. These methods can be viewed as a game between a teacher that designs challenging levels and a student that solves them. This game is potentially open-ended, leading to the co-evolution of generally-capable students. By tailoring the distribution of entire levels throughout training, adaptive curricula perform <i>unsupervised environment design</i> (UED).  However, training an environment-designing teacher is difficult, and the prior state-of-the-art UED method, <i>Prioritized Level Replay</i> (PLR), simply finds challenging levels through random search, making it unable to build off of previously found structures that were useful for training   agents in the past. We can also expect its performance to degrade as the size of the design space grows, limiting the potential for open-ended co-evolution between teacher and student. In contrast, the evolutionary processes between organisms, and their environment that UED resembles, efficiently search the design space by successively mutating a population and selecting the fittest individuals. 
    </p>

    <div class="centered-content-container">
    <figure>
      <div class="accel-maze-video-container">
        <video class="accel-maze-video" autoplay muted loop playsinline>
          <source src="dist/video/empty1.mp4" type="video/mp4">
           Your browser does not support the video tag.
        </video>

        <video class="accel-maze-video" autoplay muted loop playsinline>
          <source src="dist/video/empty2.mp4" type="video/mp4">
           Your browser does not support the video tag.
        </video>

        <video class="accel-maze-video" autoplay muted loop playsinline>
          <source src="dist/video/empty3.mp4" type="video/mp4">
           Your browser does not support the video tag.
        </video>
      </div>
      <figcaption>Figure 1. Examples of levels generated by ACCEL from an initially empty maze.</figcaption>
    </figure>
    </div>

    <p>
    We combine evolution with a principled regret-based environment designer in an algorithm called <i>Adversarially Compounding Complexity by Editing Levels</i> (ACCEL). As in PLR, our new method uses the difference between how well an agent did in an environment and how well it could have done (a metric known in decision theory as regret) to quantify the learning potential of revisiting this environment in the future. The important difference from PLR is that ACCEL evolves a curriculum by making small edits (e.g. mutations) to previously high regret levels, thus constantly producing new levels at the frontier of the student agent's capabilities. By compounding complexity over time, ACCEL harnesses the potential for efficient, open-ended search inherent to evolution. Our results show that ACCEL’s adaptive curricula lead to RL agents that outperform existing auto-curriculum learning methods.
    </p>

    <p>
    The rest of this article describes ACCEL in more technical detail. We first provide the necessary background on the UED formalism, followed by a comprehensive description of the ACCEL algorithm. We then present our experimental results in two challenging domains consisting of maze navigation and continuous-control bipedal locomotion, where ACCEL evolves levels with higher complexity and produces policies exhibiting significantly improved generalization compared to other adaptive curriculum methods.  
    </p>

    <h2>Background</h2>

    <h3>Unsupervised Environment Design</h3>

    <div class="centered-content-container">
      <figure id="ued-overview">
      <img src="dist/img/ued-overview.svg"/>
      <figcaption>Figure 2. A high-level overview of unsupervised environment design (UED): A teacher seeks to continually challenge a student agent by adapting a distribution over environment levels. As the student improves, the teacher must explore the space of possible challenges, resulting in a co-evolutionary dynamic leading to more robust agents.</figcaption>
      </figure>
    </div>

    <p>
    The RL problem is typically formalized as an agent trying to solve a Markov Decision Process (MDP), which is defined as a tuple $$\langle S, A, \mathcal{T}, \mathcal{R}, \gamma \rangle$$, where $$S$$ and $$A$$ stand for the sets of states and actions respectively and $$\mathcal{T}: S \times A \rightarrow \mathbf{\Delta}(S)$$ is a transition function representing the probability that the system transitions from a state $$s_t \in S$$ to $$s_{t+1} \in S$$ given action $$a_t \in A$$. Each transition also induces an associated reward $$r_t$$ generated by a reward function $$\mathcal{R}: S \rightarrow \mathbb{R}$$, and $$\gamma$$ is a discount factor. When provided with an MDP, the goal of RL is to learn a policy $$\pi$$ that maximizes expected discounted reward, i.e. $$\mathbb{E}\left[\sum_{i=0}^{T} r_t\gamma^t\right]$$. 
    </p>

    <p>
    Despite the generality of the MDP framework, it is often an unrealistic model for real-world environments. First, it assumes full observability of the state, which is often impossible in practice. This is addressed in partially-observable MDPs, or POMDPs, which include an observation function $$\mathcal{I}: S \rightarrow O$$ which maps the true state (which is unknown to the agent) to a potentially noisy set of observations $$O$$. Secondly, the traditional MDP framework assumes a single reward and transition function, which are fixed throughout learning. Instead, in the real world, agents may experience variations not seen during training, which makes it crucial that policies are capable of robust transfer.
    </p>

    <p>
    To address these issues, we use the recently introduced <i>Underspecified POMDP</i>, or UPOMDP<d-cite bibtex-key="paired"></d-cite>, given by $$\mathcal{M} = \langle A, O, \Theta, S^{\mathcal{M}}, \mathcal{T}^{\mathcal{M}},\mathcal{I}^{\mathcal{M}},\mathcal{R}^{\mathcal{M}},\gamma \rangle$$. This definition is identical to a POMDP with the addition of $$\Theta$$ to represent the free parameters of the environment, similar to the context in a Contextual MDP<d-cite bibtex-key="modi2017markov"></d-cite>. These parameters can be distinct at every time step and incorporated into the transition function $$\mathcal{T}^{\mathcal{M}}: S \times A \times \Theta \rightarrow \mathbf{\Delta}(S)$$. Following Jiang et al, 2021<d-cite bibtex-key="jiang2021robustplr"></d-cite> we define a <i>level</i> $$\mathcal{M}_{\theta}$$ as an environment resulting from a fixed $$\theta \in \Theta$$. We define the value of $$\pi$$ in $$\mathcal{M}_{\theta}$$ to be
    $$V^{\theta}(\pi) = \mathbb{E}\left[\sum_{i=0}^{T} r_t\gamma^t\right]$$ where $$r_t$$ are the rewards achieved by $$\pi$$ in $$\mathcal{M}_{\theta}$$. UPOMDPs benefit from their generality, since $$\Theta$$ can represent possible dynamics or changes to observations (e.g. in sim2real<d-cite bibtex-key="peng2017dr, rubiks_cube, dexterity"></d-cite>), different reward functions, or differing game maps in procedurally generated environments.
    </p>

    <h3>Methods for Unsupervised Environment Design</h3>
    <p>
    Unsupervised Environment Design (UED)<d-cite bibtex-key="paired"></d-cite> seeks to produce a series of levels that form a curriculum for a <i>student</i> agent, such that the student agent is capable of systematic generalization across all possible levels. UED typically views levels as produced by a generator (or <i>teacher</i>) maximizing some utility function $$U_t(\pi, \theta)$$, for example constant utility for a DR generator:
    </p>

    <span id="eq:dr_utility"></span>
    <d-math block>
    \begin{aligned}
    U_t^U(\pi, \theta) = C.
    \tag{1}
    \end{aligned}
    </d-math>

    <p>
    for any constant $$C$$. Recent approaches proposed to use objectives seeking to maximize <i>regret</i>, defined as the difference between the expected return of the current policy and the optimal policy, i.e:
    </p>

    <d-math block>
      \begin{align}
      U_t^R(\pi, \theta) & =\underset{\pi^* \in \Pi}{\arg\max}\;{\text{Regret}^{\theta}(\pi,\pi^*)} \tag{2} \\
      & = \underset{\pi^* \in \Pi}{\arg\max}\;\{V^\theta(\pi^*)-V^\theta(\pi)\}. \tag{3}
      \end{align}
    </d-math>

    <p>
    Regret-based objectives are desirable as they have been shown to promote the simplest possible levels that the agent cannot currently solve in a range of settings<d-cite bibtex-key="paired, jiang2021robustplr"></d-cite>. More formally, if $$S_t= \Pi$$ is the strategy set of the student and $$S_t = \Theta$$ is the strategy set of the teacher, and if the learning process reaches a Nash equilibrium, then the resulting student policy $$\pi$$ provably converges to a minimax regret policy, defined as:
    </p>

    <d-math block>
    \pi = \underset{{\pi_A \in \Pi}}{\arg\min}\{\max_{\theta,\pi_B \in \Theta , \Pi}\{\text{Regret}^{\theta}(\pi_A,\pi_B)\}\}. \tag{4}
    </d-math>

    <p>
    However, without access to $$\pi^*$$, UED algorithms must approximate the regret. PAIRED estimates regret as the difference in return attained by the main student agent and a second agent. By maximizing this difference, the teacher maximizes an approximation of the student's regret. Furthermore, multi-agent learning systems may not always converge in practice <d-cite bibtex-key="mazumdarrs20"></d-cite>. Indeed, the Achilles' heel of prior UED methods, like PAIRED<d-cite bibtex-key="paired"></d-cite>, is the difficulty of training the teacher, typically entailing an RL problem with sparse rewards and long-horizon credit assignment. An alternative regret-based UED approach is <i>Prioritized Level Replay</i> (PLR)<d-cite bibtex-key="plr, jiang2021robustplr"></d-cite>. PLR trains the student on challenging levels found by curating a rolling buffer of the highest-regret levels surfaced through random search over possible level configurations. In practice, PLR has been found to outperform other UED methods that directly train a teacher. PLR approximates regret using the <i>positive value loss</i>, given by
    </p>

    <span id="eq:pvl"/>
    <d-math block>
    \frac{1}{T}\sum_{t=0}^{T} \max \left(\sum_{k=t}^T(\gamma\lambda)^{k-t}\delta_k, 0\right), \tag{5}
    </d-math>

    <p>
    where $$\lambda$$ and $$\gamma$$ are the Generalized Advantage Estimation (GAE)<d-cite bibtex-key="schulman2016gae"></d-cite> and MDP discount factors respectively, and $$\delta_t$$, the  TD-error at timestep $$t$$. Equipped with this method for approximating regret, Corollary 1 in Jiang et al, 2021<d-cite bibtex-key="jiang2021robustplr"></d-cite> finds that if the student agent only trains on curated levels, then it will follow a minimax regret strategy at equilibrium. Thus, counterintuitively, the student learns more effectively by training on less data.
    </p>

    <p>
    Empirically PLR has been shown to produce policies with strong generalization capabilities, but remains limited in being able to only curate randomly sampled levels. PLR's inability to build off of previously discovered structures may make it challenging to sample complex structures with the frequency required to train agents capable of solving them. Further, random search will suffer from the curse-of-dimensionality in higher-dimensional design spaces, where randomly encountering levels at the frontier of the agent's current capabilities can be highly unlikely.them.
    </p>

    <h2>ACCEL: Adversarially Compounding Complexity by Editing Levels</h2>

    <div class="centered-content-container">
      <figure>
        <img id="figure-accel-overview" src="dist/img/accel-overview.svg"/>
        <figcaption>Figure 3. An overview of ACCEL. Levels are randomly sampled from a generator and evaluated, with high-regret levels added to the level replay buffer. The curator selects levels to replay, and the student only trains on replay levels. After training, the replayed levels are edited and evaluated again to compute regret for level replay</figcaption>
      </figure>
    </div>

    <p>
    In this section we introduce a new algorithm for UED, combining an evolutionary environment generator with a principled regret-based curator. Unlike PLR which relies on random sampling to produce new batches of training levels, we instead propose to make <i>edits</i> (e.g. mutations) to previously curated ones. Evolutionary methods have been effective in a variety of challenging optimization problems<d-cite bibtex-key="neuronature, qdnature"></d-cite>, yet typically rely on handcrafted, domain-specific rules. For example, POET manually filters BipedalWalker levels to have a return in the range $$[50,300]$$. The key insight in this work is that regret serves as a domain-agnostic fitness function for evolution, making it possible to consistently produce batches of levels at the frontier of agent capabilities across domains. Indeed, by iteratively editing and curating the resulting levels, the levels in the level replay buffer quickly increase in complexity. As such, we call our method <i>Adversarially Compounding Complexity by Editing Levels</i>, or ACCEL. A high-level illustration of ACCEL is depicted in <a href="#figure-accel-overview">Figure 3</a>.
    </p>

    <p>
    ACCEL does not rely on a specific editing mechanism, which could be any mutation process used in other open-ended evolutionary approaches<d-cite bibtex-key="chromaria"></d-cite>. In this paper, editing involves making a handful of changes (e.g. adding or removing obstacles in a maze), which can operate directly on environment elements within the level or on a more indirect encoding such as the latent-space representation of the level under a generative model of the environment. In general, editing may rely on more advanced mechanisms, such as search-based methods, but in this work we predominantly make use of simple, random mutations.  ACCEL makes the key assumption that regret varies smoothly with the environment parameters $$\Theta$$, such that the regret of a level is close to the regret of others within a small edit distance. If this is the case, then small edits to a single high-regret level should lead to the discovery of entire batches of high-regret levels---which could be an otherwise challenging task in high-dimensional design spaces.
    </p>

    <p>
    Following Robust PLR<d-cite bibtex-key="jiang2021robustplr"></d-cite> we do not immediately train on edited levels. Instead, we first evaluate them and only add them to the level replay buffer if they have high regret, estimated by positive value loss (<a href="#eq:pvl">Equation 5</a>). We consider two different criteria for selecting which replayed levels to edit: Under the "easy" criterion, we edit those levels which the agent can currently solve with low regret, approximated as the agent's return minus its regret. Under the "batch" criterion, we simply edit the entire batch of replayed levels.The full is procedure detailed in <a href="#accel-algo">Algorithm 1</a> below:
    </p>

    <p>
      <div id="accel-algo-container">
        <img id="accel-algo" srcset="dist/img/accel_algo.svg"></img>
      </div>
    </p>

<!--     <p>
      <div id="accel-algo-container-2">
        <span class="algo-define">Input:</span> Buffer size $$K$$, initial fill ratio $$\rho$$, level generator $$G$$
        <br/>
        <span class="algo-define">Initialize:</span> Initialize policy $$\pi(\phi)$$, level buffer $$\mathbf{\Lambda}$$
        <br/>
        Sample $$K\rho$$ levels.
        <br/>
        <span class="algo-ctrl">while</span> not converged <span class="algo-ctrl">do:</span>
      </div>
    </p>
 -->
    <p>
    ACCEL can be seen as a UED algorithm taking a step toward open-ended evolution<d-cite bibtex-key="stanley2017open"></d-cite>, where the evolutionary fitness is estimated regret, as levels only stay in the population (that is, the level replay buffer) if they meet the high-regret criterion for curation. However, ACCEL avoids some important weaknesses of evolutionary algorithms such as POET: First, ACCEL maintains a population of levels, but not a population of agents. Thus, ACCEL requires only a single desktop GPU for training. In contrast, evolutionary approaches typically require a CPU cluster. Moreoever, forgoing an agent population allows ACCEL to avoid the agent selection problem. Instead, ACCEL directly trains a single generally-capable agent. Finally, since ACCEL uses a minimax <i>regret</i>  objective (rather than minimax as in POET), it naturally promotes levels at the frontier of agent's capabilities, without relying on domain-specific knowledge (such as reward ranges). Training on high regret levels also means that ACCEL inherits the robustness guarantees in equilbrium from PLR (from Corrollary 1 in Jiang et al 2021<d-cite bibtex-key="jiang2021robustplr"></d-cite>):
    </p>

    <p>
    <b>Remark 1.</b> <i>If ACCEL reaches a Nash equilibrium, then the student follows a minimax regret strategy.</i>
    </p>

    <p>
    In stark contrast, other evolutionary approaches primarily justify their applicability solely via empirical results on specific domains. As we will show next, a key strength of ACCEL is its generality, as it can produce highly capable agents in a diverse range of environments, without domain knowledge. 
    </p>

    <h2>Experiments</h2>

    <p>
    We compare ACCEL to the top-performing UED baselines in a diverse range of environments spanning both discrete and continuous control. First we show ACCEL can produce agents that can solve complex mazes, in a fully observable lava grid requiring safe exploration and in a larger, partially-observable maze domain. In both environments, ACCEL's curriculum begins with a simple empty room, and the editor makes incremental changes by adding or removing tiles at random. As we see in <a href="#accel-lavagrid-levels">Figure 4</a>, this approach leads to the rapid emergence of highly complex structures and an agent that can solve them.
    </p>

    <div class="centered-content-container">
    <figure id="accel-lavagrid-levels">
      <div class="accel-lavagrid-video-container">
        <video class="accel-lavagrid-video" autoplay muted loop playsinline>
          <source src="dist/video/lava1.mp4" type="video/mp4">
           Your browser does not support the video tag.
        </video>

        <video class="accel-lavagrid-video" autoplay muted loop playsinline>
          <source src="dist/video/lava2.mp4" type="video/mp4">
           Your browser does not support the video tag.
        </video>

        <video class="accel-lavagrid-video" autoplay muted loop playsinline>
          <source src="dist/video/lava3.mp4" type="video/mp4">
           Your browser does not support the video tag.
        </video>

        <video class="accel-lavagrid-video" autoplay muted loop playsinline>
          <source src="dist/video/lava4.mp4" type="video/mp4">
           Your browser does not support the video tag.
        </video>
      </div>
      <figcaption>Figure 4. Examples of Lava Grid levels generated by ACCEL from an initially empty room. Note that the agent can move diagonally in this environment.</figcaption>
    </figure>
    </div>

    <p>
    Interestingly, the agent not only learns to successfully navigate levels featuring complex structures, but also is able to perform zero-shot to a series of challenging human designed tasks in the same domain. In the maze domain, we evaluate agents on the full set of human-designed test levels from previous works<d-cite bibtex-key="paired, jiang2021robustplr"></d-cite>. The results summarized in <a href="#minigrid-results">Figure 5</a> show that ACCEL produces a highly capable agent that consistently outperforms all other methods in zero-shot transfer. 
    </p>

    <!-- Insert video of ACCEL on bipedal walker -->

    <div class="centered-content-container">
      <figure>
        <img id="minigrid-results" src="dist/img/minigrid_results.svg"/>
        <figcaption>Figure 5. Zero-shot transfer performance of each method to human-designed mazes.</figcaption>
      </figure>
    </div>

    <p>
    Given ACCEL's strong performance on previous zero-shot transfer environments, we were curious to see if the robust behaviors it induces would generalize to even more complex settings. To test this, we used a $$51 \times 51$$ procedurally-generated maze environment, representing a significant increase in size from the $$15 \times 15$$ mazes seen during training. This larger maze environment also subjects the agent to a $$50\times$$ longer episode length. Once again the ACCEL agents outperforms the other methods, achieving over $$50\%$$ solved rate over 100 trials. The next best agent was PLR achieves a $$25\%$$, while all other baselines fail to solve the maze. A trajectory taken by an ACCEL agent in this large procedurally-generated maze is provided in <a href="#perfect-maze-large-video">Figure 6</a>, where the agent seems to be approximately follow the left-hand rule, a algorithm that consistently solves single-component "perfect mazes" like this one.
    </p>

    <div class="centered-content-container">
      <figure class="centered-figure">
          <video id="perfect-maze-large-video" autoplay muted loop playsinline>
            <source src="dist/video/perfect_maze_large.mp4" type="video/mp4">
             Your browser does not support the video tag.
          </video>
        <figcaption>Figure 6. An ACCEL agent solving a $$51 \times 51$$ single-component "perfect maze."</figcaption>
      </figure>
    </div>

    <p>
    Next, we move to a continuous-control BipedalWalker environment, popularized by the influential series papers on POET<d-cite bibtex-key="poet, enhanced_poet"></d-cite>. In this domain, the environment design space is an 8-dimensional indirect encoding representing the minimum and maximum intensity of four kinds of terrain-based obstacles for a two-legged robot: the roughness of the ground, height of stump obstacles, width of pit gap obstacles, and the size of ascending and descending flights of stairs. We use the same values as in POET<d-cite bibtex-key="poet"></d-cite>. In this domain, ACCEL starts its evolutionary process from perfectly flat tracks without any obstacles. As we see in <a href="bipedal-level-evolution">Figure 7</a>, ACCEL quickly compounds multiple edits leading to training levels that rapidly increase in complexity while targeting the agent's current weaknesses, resulting in a robust agent capable of solving levels with highly challenging terrain. 
    </p>

    <div class="centered-content-container">
      <figure>
          <video id="bipedal-level-evolution" autoplay muted loop playsinline>
            <source src="dist/video/accel_bipedal_evolution.mp4" type="video/mp4">
             Your browser does not support the video tag.
          </video>
        <figcaption>Figure 7. ACCEL quickly co-evolves levels of increasing complexity with an agent that learns to solve them. Left: A radar plot showing the evolution level complexity along the principle axes summarizing the challenges in the environment during a single training run. Right: The corresponding levels sampled during training, followed by a visualization of the agent's actual trajectory on a generated level.</figcaption>
      </figure>
    </div>

    <p>
    Our primary objective is to produce a generally-capable agent robust to all challenges within a specific domain. To analyze our agents' strengths and weaknesses along the main kinds of challenges in this domain, we evaluate all agents on separate environments that present each type of terrain obstacle in insolation. We also evaluate each agent on the BipedalWalkerHardcore environment, which presents a difficult configuration of the 8D environment parameter that produces a combination of type of terrain obstacle. These results, summarized in <a href="#bipedal-zs-challenges">Figure 8</a>, show that ACCEL produces agents with more robust behaviors across the full spectrum of environment challenges compared to other methods. The <a href="#accel-demo">interactive demo</a> at the start of this article allows you to play the role of environment designer, by adjusting the environment parameters to produce new levels for the ACCEL and baseline agents to solve.
    </p>

    <div class="centered-content-container">
      <figure id="bipedal-zs-challenges">
        <img src="dist/img/bipedal-zs-challenges.svg"/>
        <figcaption>Figure 8. Zero-shot transfer performance of each method in BipedalWalkerHardcore and environments testing the individual terrain challenges. The mean and standard error over 10 training seeds are shown.</figcaption>
      </figure>
    </div>

    <p>
    Despite their differing objectives, we compared the performance of agents trained with ACCEL to those produced by POET. We performed this comparison using the lower dimensional design space used by the original POET experiments<d-cite bibtex-key="poet"></d-cite>, which does not include stairs in the encoding. We trained the ACCEL agent for 50,000 student PPO updates, equivalent to approximately 3B environment steps—well under the approximately 500B environment steps used to train POET. We then looked at compared the proportion of levels in ACCEL's level replay buffer that fall into each of the difficulty settings defined in the original POET experiments: easy, challenging, very challenging, and extremely challenging levels satisfy zero, one, two, and three of the criteria in <a href="#table-bipedal-difficulty-criteria">Table 1</a>. The proportion of ACCEL's level replay buffer falling into each difficulty class at various stages of training is summarized in <a href="#level-buffer-composition">Figure 9</a>. The composition of the level replay buffer shows ACCEL is capable of discovering "extremely challenging" levels like those discovered by POET, alongside an agent that can solve them, despite using neither a population nor a domain-specific novelty criteria, as required by POET. 
    </p>

    <p>
    <table class="table" id="table-bipedal-difficulty-criteria">
      <thead>
        <tr>
          <th scope="col">Max stump height</th>
          <th scope="col">Max pit gap width</th>
          <th scope="col">Roughness</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>$$\ge 2.4$$</td>
          <td>$$\ge 6.0$$</td>
          <td>$$\ge 4.5$$</td>
        </tr>
      </tbody>
    </table>
    <figcaption>Table 2: A summary of the criteria used in the original POET experiments for determining the difficulty class for a specific environment parameterization.</figcaption>
    </p>


    <div class="centered-content-container">
      <figure id="level-buffer-composition">
        <img src="dist/img/level-buffer-composition.svg"/>
        <figcaption>Figure 9. Percentage of the ACCEL level replay buffer falling into each difficulty category. Note the complexity is purely emergent in the pursuit of high-regret levels</figcaption>
      </figure>
    </div>

    <p>
    Importantly, the goal of our work differs from that of POET. The primary motivation of ACCEL is to produce a single robust agent that can solve a wide range of challenges. ACCEL's regret-based curriculum seeks to prioritize the simplest levels the agent cannot currently solve. In contrast, POET co-evolves agent-environment pairs in order to find specialized policies for solving a single highly specialized task. POET's "specialist" agents may likely learn to solve challenging environments outside the capabilities of ACCEL's "generalist" agents, but at the cost of potentially overfitting to their paired levels. Thus, unlike ACCEL, the policies produced by POET should not be expected to be robust across the full distribution of levels. The relative strengths of POET and ACCEL highlight an important trade-off between specialization and generalization.

    While demonstrated that adaptive curricula resulting from regret-based evolutionary search can lead to highly robust generalist agents within specific domains, these results certainly do not cast doubt on the value of agent populations. It is likely that as we move to evermore challenging environments, population of specialists will become increasingly important. A population of specialist agents may be more effective at discovering diverse and complex behaviors. The interplay between generalists and specialists serves as an intringuing open question that warrants much further thought and exploration.
    </p>

    <h2>Related Work</h2>

    <p>
    <table class="table" id="table:ued_methods_summary">
      <thead>
        <tr>
          <th scope="col">Algorithm</th>
          <th scope="col">Generation strategy</th>
          <th scope="col">Generator objective</th>
          <th scope="col">Curator objective</th>
          <th scope="col">Setting</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>POET</td>
          <td>Evolution</td>
          <td>Minimax</td>
          <td>MCC</td>
          <td>Population-based</td>
        </tr>
        <tr>
          <td>PAIRED</td>
          <td>RL</td>
          <td>Minimax regret</td>
          <td>None</td>
          <td>Single agent</td>
        </tr>
        <tr>
          <td>PLR</td>
          <td>Random</td>
          <td>None</td>
          <td>Minimax regret</td>
          <td>Single agent</td>
        </tr>
        <tr id="accel-tr">
          <td>ACCEL</td>
          <td>Random + Evolution</td>
          <td>Minimax regret</td>
          <td>Minimax regret</td>
          <td>Single agent</td>
        </tr>
      </tbody>
    </table>
    <figcaption>Table 1: A summary of various UED methods in terms of their primary components.</figcaption>
    </p>

    <p>
    In this section we discuss related work, with a summary of the most closely related methods in <a href="#table:ued_methods_summary">Table 2</a>. Our paper focuses on testing agents on distributions of environments, long known to be crucial for evaluating the generality of RL agents<d-cite bibtex-key="whiteson2009generalized"></d-cite>. The inability of deep RL agents to reliably generalize has drawn considerable attention<d-cite bibtex-key="packer2019assessing,igl2019generalization, procgen, agarwal2021contrastive, zhang2018generalizationgrid, ghosh2021generalization"></d-cite>, with policies often failing to adapt to changes in the observation<d-cite bibtex-key="observational_overfitting"></d-cite>, dynamics<d-cite bibtex-key="ball2021augwm"></d-cite>, or <nobr>reward<d-cite bibtex-key="zhang2018generalizationcont"></d-cite></nobr>.
    </p>

    <p>
    This work focuses on the unsupervised environment design (UED) paradigm<d-cite bibtex-key="paired"></d-cite>, which aims to design environment directly, such that the they induce experiences that result in learning the most robust policies. Domain Randomization (DR)<d-cite bibtex-key="evolutionary_dr, cad2rl"></d-cite> can be viewed as the most basic form of UED. DR has been particularly successful in areas such as robotics<d-cite bibtex-key="tobin_dr, james2017transferring, dexterity, rubiks_cube"></d-cite>, with extensions that actively update the DR distribution<d-cite bibtex-key="adr2020, adr2_2020"></d-cite>. This paper directly extends PLR<d-cite bibtex-key="plr, jiang2021robustplr"></d-cite>, a method for curating DR levels such that those with high learning potential can be revisited by the student agent for training. PLR is related to TSCL<d-cite bibtex-key="tscl"></d-cite>, self-paced learning<d-cite bibtex-key="selfpace2019klink, space"></d-cite>, and ALP-GMM<d-cite bibtex-key="portelas2019teacher"></d-cite>, which seek to maintain and update distributions over environment parameterizations based on the  recent performance of the agent. Recently, a method similar to PLR was shown to be capable of producing generally capable agents in a simulated game world with a smooth space of levels<d-cite bibtex-key="xland"></d-cite>.
    </p>

    <p>
    Dennis et al, 2020<d-cite bibtex-key="paired"></d-cite> first formalized UED and introduced the PAIRED algorithm, a minimax regret UED algorithm whereby an environment adversary learns to present levels that maximize <i>minimax regret</i><d-cite bibtex-key="minimax_regret"></d-cite>, approximated as the difference in performance between the main student agent and a second agent. PAIRED produces agents with improved zero-shot transfer to unseen environments and has been extended to train agents that can robustly navigate websites<d-cite bibtex-key="gur2021adversarial"></d-cite>. Adversarial objectives have also been used in robotics<d-cite bibtex-key="pinto2017advrobotics"></d-cite> and in directly searching for situations in which the agent sees the weakest performance<d-cite bibtex-key="everett2019worlds"></d-cite>. POET<d-cite bibtex-key="poet, enhanced_poet"></d-cite> considers co-evolving a popualation of minimax environments alongside the agents. We take inspiration from the evolutionary nature of POET but train a single agent, which is beneficial as it takes significantly fewer resources, while also eliminating the agent selection problem. 
    </p>

    <p>
    UED is inherently related to the field of Automatic Curriculum Learning (ACL)<d-cite bibtex-key="portelas2020automatic, florensa2017, riac2009"></d-cite>, which seeks to provide an adaptive curriculum of increasingly challenging tasks or goals given a (typically) fixed environment<d-cite bibtex-key="her2017"></d-cite>. Asymmetric Self-Play<d-cite bibtex-key="sukhbaatar2018asp"></d-cite> takes the form of one agent proposing goals for another, which was shown to be effective for challenging robotic manipulation tasks<d-cite bibtex-key="openai2021asymmetric"></d-cite>. AMIGo<d-cite bibtex-key="amigo"></d-cite> and APT-Gen<d-cite bibtex-key="fang2021adaptive"></d-cite> provide solutions to problems where the target task is known, providing a curriculum of increasing difficulty. Indeed, many ACL methods emphasize learning to reach goals or states with high uncertainty<d-cite bibtex-key="racaniere2020automated, skewfit2020, frontier_zhang2020"></d-cite>, either using generative<d-cite bibtex-key="goalgan"></d-cite> or world models<d-cite bibtex-key="lexa2021"></d-cite>. Unlike these methods, UED approaches seek to fully specify environments, rather than just goals within a largely fixed environment. 
    </p>

    <p>
    Environment design has also been considered in the symbolic AI community as a means to shape an agent's decisions<d-cite bibtex-key="zhang2008ed,zhang2009ed"></d-cite>. Automated environment design<d-cite bibtex-key="keren2017equi, keren2019efficient"></d-cite> seek to redesign environments to improve agents. Unlike these works, ACCEL automatically designs environments to produce a curriculum for a student agent. 
    </p>

    <p>
    Our work also relates to the field of procedural content generation (PCG)<d-cite bibtex-key="pcg, pcg_illuminating"></d-cite>, which seeks to algorithmically generate environment levels. Popular PCG environments used in RL include the Procgen Benchmark<d-cite bibtex-key="procgen"></d-cite>, MiniGrid<d-cite bibtex-key="gym_minigrid"></d-cite>, Obstacle Tower<d-cite bibtex-key="obstacletower"></d-cite>, GVGAI<d-cite bibtex-key="perez2019general"></d-cite> and the NetHack Learning Environment<d-cite bibtex-key="kuettler2020nethack"></d-cite>. This work uses the recently proposed MiniHack environment<d-cite bibtex-key="samvelyan2021minihack"></d-cite>, which provides a flexible framework for creating diverse environments. Within the PCG community, automatically generating game levels has been of interest for more than a decade<d-cite bibtex-key="togelius2008evolving"></d-cite>, with recent methods making use of machine learning<d-cite bibtex-key="pcgml, bhaumik2020treesv, pcg"></d-cite>. PCGRL<d-cite bibtex-key="pcgrl,controllablepcgrl2021earle"></d-cite> framed level design as an RL problem, designing environments by making incremental changes. Unlike ACCEL, it makes use of hand-designed dense rewards, and focuses on the design of levels for human players, rather than our goal of training of a generally-capable RL agent without domain-specific feedback.
    </p>

    <h2>Conclusion and Future Work</h2>

    <p>
    This article presented ACCEL, a new method for Unsupervised Environment Design (UED) that evolves a curriculum by editing previously discovered high-regret levels. This makes it possible to generate a wide variety of levels at the frontier of the agent's capabilities, producing curricula that start simple and quickly compound. ACCEL provides a principled regret-based curriculum that does not require domain-specific heuristics, alongside an evolutionary process that produces a broad spectrum of complexity matched to the agent's current capabilities. In our experiments we showed that ACCEL is capable of producing robust agents in a several challenging design spaces, where ACCEL agents outperform the best-performing baselines. 
    </p>

    <p>
    We are excited by the many possibilities for extending how ACCEL edits and maintains its population of high-regret levels. The editing mechanism could encompass a wide variety of approaches, such as Neural Cellular Automata<d-cite bibtex-key="earle2021illuminating"></d-cite>, using controllable editors<d-cite bibtex-key="controllablepcgrl2021earle"></d-cite>, or perturbing the latent space of a generative model. Another possibility is to actively search for levels that are likely to lead to useful levels in the future<d-cite bibtex-key="evolvabilityes">. There is also much promise in methods that directly encourage levels diversity, which we expect to play a more important role as the level design space grows. We believe the intersection of evolution and adaptive curricula presents many promising directions that may take us closer to producing a truly open-ended learning process between the agent and the environment<d-cite bibtex-key="stanley2017open"></d-cite>. 
    </p>
  </d-article>

  <d-appendix>

      <h3>Acknowledgements</h3>

      <p>
      We thank Kenneth Stanley, Alessandro Lazaric, Ian Fox and Iryna Korshunova for useful discussions and feedback on this work, while we are also grateful to anonymous reviewers for their recommendations that helped improve the paper.
      </p>

      <h3>Citation</h3>
      <p>
      For attribution in academic contexts, please cite this work as
      </p>

      <pre class="citation short">Parker-Holder et al., "Evolving Curricula with Regret-Based Environment Design", 2022.</pre>

      <p>BibTeX citation</p>
      <pre class="citation long">
@article{parkerholder2022evolving,
  title={Evolving Curricula with Regret-Based Environment Design},
  author={Parker-Holder, Jack and Jiang, Minqi and Dennis, Michael D and Samvelyan, Mikayel and Foerster, Jakob Nicolaus and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:2203.01302},
  year={2022}
}</pre>

    <d-bibliography src="dist/bibliography.bib"></d-bibliography>
  </d-appendix>

  <!-- <distill-footer></distill-footer> -->

</body>